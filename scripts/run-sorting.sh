#!/bin/bash
# Script d'exÃ©cution du tri MapReduce sur le cluster Hadoop
# Usage: ./scripts/run-sorting.sh

set -e

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  MapReduce Sorting - ExÃ©cution sur Cluster Hadoop"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Couleurs
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# Configuration
MASTER_CONTAINER="hadoop-master"
INPUT_FILE="numbers.txt"
HDFS_INPUT_DIR="/user/root/input"
HDFS_OUTPUT_DIR="/user/root/output"
MAPPER_FILE="mapper.py"
REDUCER_FILE="reducer.py"

# VÃ©rifier que le conteneur master tourne
echo "1ï¸âƒ£  VÃ©rification du cluster..."
if ! docker ps | grep -q $MASTER_CONTAINER; then
    echo -e "${RED}âŒ Le conteneur $MASTER_CONTAINER n'est pas dÃ©marrÃ©${NC}"
    echo "   Lancez d'abord: docker-compose up -d"
    exit 1
fi
echo -e "${GREEN}âœ“ Cluster Hadoop actif${NC}"
echo ""

# VÃ©rifier que Hadoop est dÃ©marrÃ©
echo "2ï¸âƒ£  VÃ©rification de Hadoop..."
NAMENODE_STATUS=$(docker exec $MASTER_CONTAINER jps | grep -c "NameNode" || echo "0")
if [ "$NAMENODE_STATUS" -eq "0" ]; then
    echo -e "${YELLOW}âš ï¸  Hadoop n'est pas dÃ©marrÃ©. DÃ©marrage...${NC}"
    docker exec $MASTER_CONTAINER ./start-hadoop.sh
    echo "   Attente de 30 secondes pour l'initialisation..."
    sleep 30
fi
echo -e "${GREEN}âœ“ Hadoop opÃ©rationnel${NC}"
echo ""

# Copier les fichiers Python dans le conteneur
echo "3ï¸âƒ£  Copie des scripts MapReduce..."
docker cp src/python/$MAPPER_FILE $MASTER_CONTAINER:/root/$MAPPER_FILE
docker cp src/python/$REDUCER_FILE $MASTER_CONTAINER:/root/$REDUCER_FILE
echo -e "${GREEN}âœ“ Scripts copiÃ©s${NC}"
echo ""

# VÃ©rifier que le fichier de donnÃ©es existe
echo "4ï¸âƒ£  VÃ©rification des donnÃ©es..."
if [ ! -f "data/input/$INPUT_FILE" ]; then
    echo -e "${RED}âŒ Fichier data/input/$INPUT_FILE introuvable${NC}"
    echo "   GÃ©nÃ©rez les donnÃ©es avec: python src/python/generate_data.py"
    exit 1
fi

# Copier le fichier de donnÃ©es
docker cp data/input/$INPUT_FILE $MASTER_CONTAINER:/root/$INPUT_FILE
echo -e "${GREEN}âœ“ DonnÃ©es copiÃ©es ($(wc -l < data/input/$INPUT_FILE) nombres)${NC}"
echo ""

# Nettoyer les anciens rÃ©pertoires HDFS
echo "5ï¸âƒ£  PrÃ©paration de HDFS..."
docker exec $MASTER_CONTAINER bash -c "hdfs dfs -rm -r $HDFS_INPUT_DIR $HDFS_OUTPUT_DIR 2>/dev/null || true"
docker exec $MASTER_CONTAINER hdfs dfs -mkdir -p $HDFS_INPUT_DIR
echo -e "${GREEN}âœ“ RÃ©pertoires HDFS crÃ©Ã©s${NC}"
echo ""

# Charger les donnÃ©es dans HDFS
echo "6ï¸âƒ£  Chargement des donnÃ©es dans HDFS..."
docker exec $MASTER_CONTAINER hdfs dfs -put $INPUT_FILE $HDFS_INPUT_DIR/
echo -e "${GREEN}âœ“ DonnÃ©es chargÃ©es dans $HDFS_INPUT_DIR/$INPUT_FILE${NC}"
echo ""

# Lancer le job MapReduce
echo "7ï¸âƒ£  Lancement du Job MapReduce..."
echo -e "${BLUE}   (Cela peut prendre 30-60 secondes...)${NC}"
echo ""

START_TIME=$(date +%s)

docker exec $MASTER_CONTAINER bash -c "
hadoop jar \$HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \
  -input $HDFS_INPUT_DIR/$INPUT_FILE \
  -output $HDFS_OUTPUT_DIR \
  -mapper 'python3 $MAPPER_FILE' \
  -reducer 'python3 $REDUCER_FILE' \
  -file /root/$MAPPER_FILE \
  -file /root/$REDUCER_FILE
"

END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))

echo ""
echo -e "${GREEN}âœ“ Job MapReduce terminÃ© en ${DURATION}s${NC}"
echo ""

# Afficher les rÃ©sultats
echo "8ï¸âƒ£  RÃ©cupÃ©ration des rÃ©sultats..."

# CrÃ©er le rÃ©pertoire de sortie s'il n'existe pas
mkdir -p data/output

# RÃ©cupÃ©rer le fichier de rÃ©sultat
docker exec $MASTER_CONTAINER hdfs dfs -cat $HDFS_OUTPUT_DIR/part-00000 > data/output/sorted_numbers.txt

echo -e "${GREEN}âœ“ RÃ©sultats sauvegardÃ©s: data/output/sorted_numbers.txt${NC}"
echo ""

# Afficher un aperÃ§u
echo "ğŸ“Š AperÃ§u des rÃ©sultats:"
echo ""
echo "   Premiers nombres:"
head -5 data/output/sorted_numbers.txt | sed 's/^/     /'
echo "     ..."
echo "   Derniers nombres:"
tail -5 data/output/sorted_numbers.txt | sed 's/^/     /'
echo ""

# Validation
echo "9ï¸âƒ£  Validation du tri..."
if command -v python3 &> /dev/null; then
    python3 src/python/validate_sort.py data/input/$INPUT_FILE data/output/sorted_numbers.txt
else
    echo -e "${YELLOW}âš ï¸  Python non disponible, validation ignorÃ©e${NC}"
fi

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo -e "${GREEN}  âœ… Tri MapReduce terminÃ© avec succÃ¨s !${NC}"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“‚ Fichiers gÃ©nÃ©rÃ©s:"
echo "   - data/output/sorted_numbers.txt"
echo ""
echo "ğŸŒ Interfaces Web disponibles:"
echo "   - HDFS NameNode:        http://localhost:9870"
echo "   - YARN ResourceManager: http://localhost:8088"
echo "   - Worker 1:             http://localhost:8040"
echo "   - Worker 2:             http://localhost:8041"
echo ""
echo "ğŸ“Š Statistiques HDFS:"
docker exec $MASTER_CONTAINER hdfs dfs -du -h $HDFS_OUTPUT_DIR
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"